#                              

## 使用 Datasets 加载一个数据集

以下代码用于从 hub 上下载(加载)一个数据集

```python
from datasets import load_dataset

# 加载数据集
raw_datasets = load_dataset("glue", "mrpc")
print(raw_datasets)

# 得到数据集中的训练集并打印训练集中的第一对
raw_train_dataset = raw_datasets["train"]
print(raw_train_dataset[0])

# 查看训练集中每一个属性的信息
print(raw_train_dataset.features)
```

```
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}
{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}
```

从上面的结果我们可以看出一个数据集就是一个 `DatasetDict` 对象, 这个数据集中包含如下三个子集:

1. 训练集
2. 验证集
3. 测试集

而每一个集合有一个代表行数的变量(每个集合中有多少行), 另外包含如下四个属性:

1. sentence1
2. sentence2
3. label: 从最后一行的打印, 可以看出 `label` 对应一个数组中的下标: `0-not_equivalent(反义词)`, `1-equivalent(近义词)`.
   这个 `label` 的意思是 `sentence1`和`sentence2` 这两个句子是近义词还是反义词的意思.
4. idx

## 预处理训练数据

现在我们得到的数据集是不能够直接给到模型进行训练使用的, 我们需要先将它转换成模型能够理解的数字(最终结果为`张量`).
下面是一个使用 `tokenizer` 来对数据集中的 `sentence1`和`sentence2` 进行编码的过程

```python
from datasets import load_dataset
from transformers import AutoTokenizer

# 加载数据集
raw_datasets = load_dataset("glue", "mrpc")
print(raw_datasets)

# 得到数据集中的训练集并打印训练集中的第一对
raw_train_dataset = raw_datasets["train"]
print(raw_train_dataset[0])

# 查看训练集中每一个属性的信息
print(raw_train_dataset.features)

# 加载一个 tokenizer
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# raw_datasets["train"] 的结果是一个经过封装的集合(类似于 java 中的 list), 他可以以 raw_datasets["train"]["sentence1"] 这种方式寻思拿到每一行数据中的 sentence1 属性收集起来并返回一个集合
# 将训练集中所有的 sentence1和sentence2 给到 tokenizer 并进行编码
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])

inputs1 = tokenizer("This is the first sentence.", "This is the second one.")
print(inputs1)

inputs2 = tokenizer("This is the first sentence.", "This is the second one.", "Hello, world")
print(inputs2)
```

```
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}
{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}
{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [101, 7592, 1010, 2088, 102]}
```

看上面这段代码以及执行后打印的结果, 以下对代码的执行和结果进行解释

`tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])`
`tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])`

这两行代码的作用是给 `tokenizer` 传入一个字符串集合, 让 `tokenizer` 分别为集合中的每一个句子进行编码, 其返回结果(
`tokenized_sentences_1和tokenized_sentences_2`)的格式如下:

```
{

   input_ids: [
      [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
      [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
      [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102]
   ],
   token_type_ids: [
      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
   ],
   attention_mask: [
      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
   ],

}
```

`上面的结果只是为了演示一下返回值的格式, 数组中的 长度,数量,内容 都是随意编的.`

从上面的结果来看, `tokenizer` 在接收一个字符串集合的时候, 他会把每一行的字符串都进行编码(每一行编码后的结果是一个数组,
组合起来就是 `input_ids` 这个二维数组). 因为只有一行字符串, 所以 `token_type_ids` 都是 0, 代表这是第一行字符串.
因为只有一行字符串,
所以其并不需要填充, 所以 `attention_mask` 中都是 1(代表有效的, 不存在填充字符)

`inputs1 = tokenizer("This is the first sentence.", "This is the second one.")`

这一行代码的意思是给到 `tokenizer` 一个句子对(两个字符串), 让 `tokenizer` 进行编码, 编码之后的结果如下:

```
{
   'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 
   'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 
   'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

通过上面可以看出, 此时参数的定义是一个句子对, `tokenizer` 将两个句子进行编码之后放到了一个 `input_ids` 数组中,
`token_type_ids` 这个数组用来表示哪些是第一个句子(0), 哪些是第二个句子(1), 因为这一个句子对(两个字符串)最终会被拼接成一个序列,
所以 `attention_mask` 也都是 1(不需要填充或没有填充).

`inputs2 = tokenizer("This is the first sentence.", "This is the second one.", "Hello, world")`

这一句代码的返回值结果与上一句代码基本相同, 虽然不会报错, 但其从使用上来说是错误的, 因为对于 `transformers`
的句子对任务本身来说只支持两句话, , 第三个参数会被当作无名参数传给 `**kwargs`

`注: token_type_ids 是用来区分一个句子对中的两句话的, 其只有两个值, 0和1, 0 代表这是第一句话, 1 代表这是第二句话.`

`注: 如果选择其它模型(其它 checkpoint)时, 不一定有 token_type_ids, 以上代码及示例是基于 BERT 模型来进行的. 只有当模型在构建时需要时候才会出现.`

如果想将我们下载的数据集中的所有的句子对一起进行编码, 可以使用如下代码:

```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

以下是一段完成的例子:

```python
from datasets import load_dataset
from transformers import AutoTokenizer

# 加载数据集
raw_datasets = load_dataset("glue", "mrpc")
print(raw_datasets)

# 得到数据集中的训练集并打印训练集中的第一对
raw_train_dataset = raw_datasets["train"]
print(raw_train_dataset[0])

# 查看训练集中每一个属性的信息
print(raw_train_dataset.features)

# 加载一个 tokenizer
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# 对训练集中所有的句子对进行编码, 并进行填充和截断
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"], raw_datasets["train"]["sentence2"], padding=True,
                                  truncation=True)
```
